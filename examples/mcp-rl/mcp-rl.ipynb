{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OpenPipe/ART/blob/art-mcp/examples/mcp-rl/mcp-rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZYLROd8xnV"
      },
      "source": [
        "To teach a model to use your MCP server, click **Runtime** > **Run all**. Make sure you've enabled a free Tesla T4 GPU and edit the [configuration](#configuration) cell below!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [GitHub](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**MCPâ€¢RL: Teach you agent how to use any MCP server**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 3B model to effectively use any MCP server. Simply provide an MCP server url and the notebook will:\n",
        "\n",
        "1. Query the server's tools\n",
        "2. Generate a set of input tasks that use those tools\n",
        "3. Train the model on those tasks using automatic RULER evaluation\n",
        "4. Test the trained model by giving it new tasks to complete\n",
        "\n",
        "RULER judges response quality purely from the agent's final output - no labeled data required!\n",
        "\n",
        "*Note: In this notebook we use [Smithery](https://smithery.ai/) servers to keep things simple, but the technique below applies to all MCP servers!*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OsrwCDQ5cviC"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ’¿ Installation\n",
        "# Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)\n",
        "# Copyright (c) Unsloth contributors.\n",
        "# License: GNU LGPL v3.0.\n",
        "# Modifications by OpenPipe:\n",
        "# - switched to uv\n",
        "# - changed vllm/triton pinning logic\n",
        "# - added protobuf pins\n",
        "# - adjusted syntax for pushing to HF\n",
        "# See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.\n",
        "\n",
        "%%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install openpipe-art[backend]==0.4.11 tenacity \"mcp>=1.11.0\" \"gql<4\" aiohttp --prerelease allow --no-cache-dir\n",
        "else:\n",
        "    try:\n",
        "        import numpy\n",
        "\n",
        "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except:\n",
        "        get_numpy = \"numpy\"\n",
        "    try:\n",
        "        import subprocess\n",
        "\n",
        "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except:\n",
        "        is_t4 = False\n",
        "    get_vllm, get_triton = (\n",
        "        (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    )\n",
        "    !uv pip install --upgrade \\\n",
        "        openpipe-art[backend]==0.4.11 tenacity \"protobuf==5.29.5\" {get_vllm} {get_numpy} --prerelease allow --no-cache-dir\n",
        "    !uv pip install -qqq {get_triton}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      },
      "source": [
        "<a name=\"configuration\"></a>\n",
        "\n",
        "### ðŸŽ¯ Configuration - Edit These Settings\n",
        "\n",
        "Add an OpenRouter API key and Smithery MCP server url below.\n",
        "\n",
        "### **Smithery Instructions**\n",
        "\n",
        "Smithery hosts a variety of useful MCP servers. If you're not sure which to use, try the [Exa server](https://smithery.ai/server/exa), which allows your model to query data from across the web.\n",
        "\n",
        "To generate an authenticated Smithery MCP url, follow these steps:\n",
        "\n",
        "1. Sign up for a [Smithery](https://smithery.ai) account\n",
        "2. Navigate to an MCP [server](https://smithery.ai/server/exa)\n",
        "3. Get an authenticated url by clicking the orange <u>Get URL with keys instead</u> button on the right\n",
        "4. Set `SMITHERY_MCP_URL` to the generated url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "so6r1_OG9en3"
      },
      "outputs": [],
      "source": [
        "# Required - Used for generating training inputs and RULER evaluation\n",
        "OPENROUTER_API_KEY = \"\"  # Put your OpenRouter key here\n",
        "\n",
        "# ðŸ”Œ Point to any Smithery-hosted MCP server (make sure you click \"Get URL with keys instead\", otherwise this will not work)\n",
        "SMITHERY_MCP_URL = \"\"\n",
        "\n",
        "# Optional - Enables metric logging\n",
        "WANDB_API_KEY = \"\"\n",
        "\n",
        "# Choose the base model to train\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"  # Options: \"Qwen/Qwen2.5-3B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "I_AFDSOv_LrB"
      },
      "outputs": [],
      "source": [
        "# @title Advanced Settings\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"mcprl-3b-exa\"  # Name for your trained model\n",
        "PROJECT_NAME = \"mcp-rl\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 16,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 1,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 4,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "MAX_TURNS = 10  # Maximum number of turns for the model to generate during one rollout\n",
        "\n",
        "NUM_TEST_INPUTS = 8  # Number of test inputs to generate\n",
        "RULER_MODEL = \"openrouter/openai/o4-mini\"  # Model for RULER evaluation\n",
        "INPUT_GENERATION_MODEL = \"openai/o4-mini\"\n",
        "\n",
        "# GPU configuration (for T4 â€”Â keep these as-is unless you have a reason to change them)\n",
        "MAX_SEQ_LENGTH = 16384  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.7  # GPU memory usage (0.0-1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "PfXGRuhhd7hr"
      },
      "outputs": [],
      "source": [
        "# @title Debug utilities\n",
        "\n",
        "import json\n",
        "import time\n",
        "import traceback\n",
        "from typing import Any\n",
        "\n",
        "DEBUG_LOG = True  # flip to False to silence logs\n",
        "LOG_JSON_MAX = 2000  # cap large JSON prints\n",
        "\n",
        "\n",
        "def _ts() -> str:\n",
        "    return time.strftime(\"%H:%M:%S\")\n",
        "\n",
        "\n",
        "def log(msg: str, **kv):\n",
        "    if not DEBUG_LOG:\n",
        "        return\n",
        "    parts = [f\"[{_ts()}] {msg}\"]\n",
        "    if kv:\n",
        "        kv_str = \" \".join(f\"{k}={repr(v)}\" for k, v in kv.items())\n",
        "        parts.append(\"| \" + kv_str)\n",
        "    print(\" \".join(parts))\n",
        "\n",
        "\n",
        "def log_json(title: str, payload: Any, max_len: int = LOG_JSON_MAX):\n",
        "    if not DEBUG_LOG:\n",
        "        return\n",
        "    try:\n",
        "        s = json.dumps(payload, indent=2, default=str)\n",
        "    except Exception:\n",
        "        s = str(payload)\n",
        "    if len(s) > max_len:\n",
        "        s = s[:max_len] + \"\\n... (truncated)\"\n",
        "    print(f\"[{_ts()}] {title}:\\n{s}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gxUn4E_IPjq8"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ”Œ MCP helpers\n",
        "\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "import mcp.types as types\n",
        "from mcp.client.session import ClientSession\n",
        "from mcp.client.streamable_http import streamablehttp_client\n",
        "\n",
        "if not SMITHERY_MCP_URL:\n",
        "    raise ValueError(\"SMITHERY_MCP_URL is empty. Set it in the Configuration cell.\")\n",
        "\n",
        "\n",
        "@asynccontextmanager\n",
        "async def mcp_session():\n",
        "    \"\"\"\n",
        "    Connects to the remote Smithery MCP server using the full URL that includes\n",
        "    your API key & profile. No OAuth provider is used.\n",
        "    \"\"\"\n",
        "    async with streamablehttp_client(SMITHERY_MCP_URL) as (read, write, _):\n",
        "        async with ClientSession(read, write) as session:\n",
        "            await session.initialize()\n",
        "            yield session\n",
        "\n",
        "\n",
        "async def list_tools_and_resources():\n",
        "    \"\"\"Return (tools_result, resources_result) from the remote Smithery server.\"\"\"\n",
        "    async with mcp_session() as session:\n",
        "        tools = await session.list_tools()\n",
        "        try:\n",
        "            resources = await session.list_resources()\n",
        "        except Exception:\n",
        "            # Some servers don't implement resources; keep interface stable\n",
        "            class _Empty:\n",
        "                resources = []\n",
        "\n",
        "            resources = _Empty()\n",
        "        return tools, resources\n",
        "\n",
        "\n",
        "async def call_mcp_tool(tool_name: str, arguments: dict):\n",
        "    \"\"\"Invoke a tool on the remote Smithery server and return the CallToolResult.\"\"\"\n",
        "    async with mcp_session() as session:\n",
        "        return await session.call_tool(tool_name, arguments)\n",
        "\n",
        "\n",
        "tools, resources = await list_tools_and_resources()\n",
        "print(\"Tools:\", [t.name for t in tools.tools])\n",
        "print(\n",
        "    \"Resources:\",\n",
        "    [getattr(r, \"uri\", None) for r in getattr(resources, \"resources\", []) or []],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nEB1JGY6Pjq8"
      },
      "outputs": [],
      "source": [
        "# @title Let's generate our train and validation scenarios!\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Import the generate_scenarios function from art.mcp and logging utilities\n",
        "from art.mcp import generate_scenarios\n",
        "from art.mcp.generate_scenarios import preview_scenarios\n",
        "from art.utils.logging import info, ok, step, warn, err\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# required env/key check\n",
        "# If OPENROUTER_API_KEY exists as a var, use it; otherwise pull from env\n",
        "_openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "try:\n",
        "    _openrouter_key = _openrouter_key if _openrouter_key else OPENROUTER_API_KEY  # noqa: F821 (defined upstream in your notebook)\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "if _openrouter_key:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = _openrouter_key\n",
        "    ok(\"OPENROUTER_API_KEY found.\")\n",
        "else:\n",
        "    err(\"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\")\n",
        "    raise ValueError(\n",
        "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Convert MCP tools and resources to the expected format\n",
        "tools_result, resources_result = await list_tools_and_resources()\n",
        "\n",
        "# Convert tools to the format expected by generate_scenarios\n",
        "tools_list = []\n",
        "for tool in tools_result.tools or []:\n",
        "    tools_list.append({\n",
        "        \"name\": tool.name,\n",
        "        \"description\": tool.description,\n",
        "        \"parameters\": tool.inputSchema,\n",
        "    })\n",
        "\n",
        "# Convert resources to the format expected by generate_scenarios  \n",
        "resources_list = []\n",
        "for resource in getattr(resources_result, \"resources\", []) or []:\n",
        "    resources_list.append({\n",
        "        \"uri\": str(resource.uri),\n",
        "        \"name\": resource.name,\n",
        "        \"description\": resource.description,\n",
        "        \"mimeType\": resource.mimeType,\n",
        "    })\n",
        "\n",
        "# Calculate total scenarios needed\n",
        "try:\n",
        "    expected_total = TRAINING_CONFIG[\"num_training_inputs\"] + NUM_TEST_INPUTS  # noqa: F821\n",
        "except NameError:\n",
        "    err(\"TRAINING_CONFIG/NUM_TEST_INPUTS not defined in this notebook.\")\n",
        "    raise\n",
        "\n",
        "info(f\"Target total scenarios: {expected_total}\")\n",
        "\n",
        "# Generate scenarios using the art.mcp function\n",
        "max_attempts = 10\n",
        "scenarios = None\n",
        "\n",
        "for attempt in range(1, max_attempts + 1):\n",
        "    step(f\"Attempt {attempt}/{max_attempts} ...\")\n",
        "    t_attempt = time.perf_counter()\n",
        "    try:\n",
        "        scenario_collection = await generate_scenarios(\n",
        "            tools=tools_list,\n",
        "            resources=resources_list,\n",
        "            num_scenarios=expected_total,\n",
        "            show_preview=False,  # We'll preview separately for train/val\n",
        "            generator_model=INPUT_GENERATION_MODEL,\n",
        "            generator_api_key=_openrouter_key,\n",
        "        )\n",
        "        # Convert GeneratedScenarioCollection to list of dicts for compatibility\n",
        "        scenarios = [{\"task\": s.task, \"difficulty\": s.difficulty} for s in scenario_collection.scenarios]\n",
        "        ok(f\"Attempt {attempt} succeeded in {time.perf_counter() - t_attempt:.2f}s.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        warn(f\"Attempt {attempt} failed: {e}\")\n",
        "        if attempt < max_attempts:\n",
        "            time.sleep(min(1.5 * attempt, 6.0))\n",
        "        else:\n",
        "            err(\"All attempts exhausted.\")\n",
        "            raise\n",
        "\n",
        "# Split into train/val\n",
        "ok(f\"Generated {len(scenarios)} scenarios total.\")\n",
        "step(\"Shuffling scenarios and splitting into train/val ...\")\n",
        "random.shuffle(scenarios)\n",
        "\n",
        "train_n = TRAINING_CONFIG[\"num_training_inputs\"]  # noqa: F821\n",
        "raw_train_scenarios = scenarios[:train_n]\n",
        "raw_val_scenarios = scenarios[train_n:]\n",
        "\n",
        "ok(f\"Train: {len(raw_train_scenarios)} | Val: {len(raw_val_scenarios)}\")\n",
        "\n",
        "info(\"Sample (train) preview:\")\n",
        "preview_scenarios(raw_train_scenarios, n=min(5, len(raw_train_scenarios)))\n",
        "\n",
        "info(\"Sample (val) preview:\")\n",
        "preview_scenarios(raw_val_scenarios, n=min(5, len(raw_val_scenarios)))\n",
        "\n",
        "ok(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FET-_U0IPjq8"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to train your model!\n",
        "\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import weave\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "from art.rewards import ruler_score_group\n",
        "from art.utils import iterate_dataset\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Optional\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "    weave.init(PROJECT_NAME)\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)\n",
        "\n",
        "\n",
        "def get_content_text(result) -> str:\n",
        "    # Extract text content from tool call result per MCP content schema\n",
        "    if isinstance(result, str):\n",
        "        return result\n",
        "    if hasattr(result, \"content\") and result.content:\n",
        "        out = \"\"\n",
        "        for item in result.content:\n",
        "            if isinstance(item, types.TextContent):\n",
        "                out += item.text\n",
        "            else:\n",
        "                out += str(item)\n",
        "        return out\n",
        "    if hasattr(result, \"structured\") and result.structured is not None:\n",
        "        try:\n",
        "            return json.dumps(result.structured)\n",
        "        except Exception:\n",
        "            return str(result.structured)\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class McpScenario:\n",
        "    \"\"\"A scenario for MCP agent evaluation against a remote Smithery server.\"\"\"\n",
        "\n",
        "    task_description: str\n",
        "    max_turns: int = MAX_TURNS\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "async def rollout(\n",
        "    model: art.Model,\n",
        "    scenario: McpScenario,\n",
        "    debug: bool = False,\n",
        ") -> art.Trajectory:\n",
        "    \"\"\"Run an MCP agent rollout against the remote Smithery MCP server.\"\"\"\n",
        "    traj = art.Trajectory(\n",
        "        messages_and_choices=[],\n",
        "        reward=0,\n",
        "        metadata={\"task\": scenario.task_description},\n",
        "        metrics={\n",
        "            \"task_completed\": False,\n",
        "            \"success\": False,\n",
        "            \"ran_out_of_turns\": False,\n",
        "        },\n",
        "        scenario=scenario,\n",
        "    )\n",
        "\n",
        "    # Discover available tools from the remote server\n",
        "    tools_result, _resources_result = await list_tools_and_resources()\n",
        "    tool_names = [t.name for t in tools_result.tools]\n",
        "    log(\"rollout: discovered tools\", count=len(tool_names), names=tool_names)\n",
        "\n",
        "    # Convert to OpenAI tool format\n",
        "    tool_schemas = []\n",
        "    for tool in tools_result.tools:\n",
        "        tool_schema = {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": tool.name,\n",
        "                \"description\": tool.description or f\"MCP tool: {tool.name}\",\n",
        "                \"parameters\": tool.inputSchema or {\"type\": \"object\", \"properties\": {}},\n",
        "            },\n",
        "        }\n",
        "        tool_schemas.append(tool_schema)\n",
        "\n",
        "    # Add completion tool schema\n",
        "    tool_schemas.append(\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"complete_task\",\n",
        "                \"description\": \"Complete the task with a summary\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"summary\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Summary of accomplishments\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"summary\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    )\n",
        "\n",
        "    traj.tools = tool_schemas\n",
        "\n",
        "    # Initialize conversation\n",
        "    system_prompt = (\n",
        "        f\"You are an MCP (Model Context Protocol) agent.\\n\\n\"\n",
        "        f\"Use MCP tools through the server to complete your task.\\n\\n\"\n",
        "        f\"When you believe you have completed the task, call the 'complete_task' function with a summary of what you accomplished. \"\n",
        "        f\"You have a total of {scenario.max_turns} turns.\"\n",
        "        # NOTE: removing 'Only use tool calls, do not write any content.' â€” some models\n",
        "        # will freeze if they think plain text is disallowed. Let them output thoughts but\n",
        "        # we only process tool calls below.\n",
        "    )\n",
        "\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Please complete this task: {scenario.task_description}\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    num_turns = 0\n",
        "    task_completed = False\n",
        "\n",
        "    # Main interaction loop\n",
        "    while num_turns < scenario.max_turns and not task_completed:\n",
        "        num_turns += 1\n",
        "\n",
        "        try:\n",
        "            # === Log request ===\n",
        "            last_user = next(\n",
        "                (m for m in reversed(traj.messages()) if m[\"role\"] == \"user\"), None\n",
        "            )\n",
        "            log(\n",
        "                \"LLM request\",\n",
        "                step=num_turns,\n",
        "                model=(model.inference_model_name or model.name),\n",
        "                tools=len(tool_schemas),\n",
        "                last_user=(last_user[\"content\"][:160] + \"...\" if last_user else None),\n",
        "            )\n",
        "\n",
        "            # Get LLM response\n",
        "            async with traj.track_duration(\"llm_completion\"):\n",
        "                openai_client = AsyncOpenAI(\n",
        "                    api_key=model.inference_api_key,\n",
        "                    base_url=model.inference_base_url,\n",
        "                )\n",
        "\n",
        "                # We also log the request body (without huge params)\n",
        "                req_preview = {\n",
        "                    \"model\": model.inference_model_name\n",
        "                    if model.inference_model_name\n",
        "                    else model.name,\n",
        "                    \"messages_len\": len(traj.messages()),\n",
        "                    \"tools_len\": len(tool_schemas),\n",
        "                }\n",
        "                log_json(\"LLM request (preview)\", req_preview)\n",
        "\n",
        "                response = await openai_client.chat.completions.create(\n",
        "                    model=model.inference_model_name\n",
        "                    if model.inference_model_name\n",
        "                    else model.name,\n",
        "                    messages=traj.messages(),\n",
        "                    tools=tool_schemas,\n",
        "                    max_completion_tokens=8000,\n",
        "                )\n",
        "\n",
        "            # === Log response ===\n",
        "            choice = response.choices[0]\n",
        "\n",
        "            finish_reason = getattr(choice, \"finish_reason\", None)\n",
        "            msg = choice.message\n",
        "            has_tools = bool(getattr(msg, \"tool_calls\", None))\n",
        "            content_preview = (\n",
        "                (msg.content[:200] + \"...\")\n",
        "                if isinstance(msg.content, str) and msg.content\n",
        "                else str(msg.content)[:200]\n",
        "            )\n",
        "            log(\n",
        "                \"LLM response parsed\",\n",
        "                finish_reason=finish_reason,\n",
        "                has_tool_calls=has_tools,\n",
        "                content_preview=content_preview,\n",
        "            )\n",
        "\n",
        "            traj.messages_and_choices.append(choice)\n",
        "\n",
        "            # Handle tool calls\n",
        "            if msg.tool_calls:\n",
        "                for tool_call in msg.tool_calls:\n",
        "                    try:\n",
        "                        log(\n",
        "                            \"Tool call received\",\n",
        "                            name=tool_call.function.name,\n",
        "                            raw_args=tool_call.function.arguments,\n",
        "                        )\n",
        "                        tool_args = json.loads(tool_call.function.arguments or \"{}\")\n",
        "\n",
        "                        if tool_call.function.name == \"complete_task\":\n",
        "                            traj.metrics[\"task_completed\"] = True\n",
        "                            task_completed = True\n",
        "                            traj.logs.append(\n",
        "                                f\"Task completion attempted with summary: {tool_args.get('summary', '')}\"\n",
        "                            )\n",
        "                            # We still append a tool message for completeness\n",
        "                            traj.messages_and_choices.append(\n",
        "                                {\n",
        "                                    \"role\": \"tool\",\n",
        "                                    \"tool_call_id\": tool_call.id,\n",
        "                                    \"content\": \"Task marked complete.\",\n",
        "                                }\n",
        "                            )\n",
        "                        else:\n",
        "                            # ðŸ”§ Call MCP tool through remote Smithery session\n",
        "                            result = await call_mcp_tool(\n",
        "                                tool_call.function.name, tool_args\n",
        "                            )\n",
        "\n",
        "                            content_text = get_content_text(result)\n",
        "                            log(\n",
        "                                \"Tool result\",\n",
        "                                name=tool_call.function.name,\n",
        "                                len=len(content_text),\n",
        "                            )\n",
        "\n",
        "                            if len(content_text) > 20000:\n",
        "                                # print(\n",
        "                                #     f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
        "                                # )\n",
        "                                # print(f\"Args: {tool_args}\")\n",
        "                                # print(content_text[:1000])\n",
        "                                # print(content_text[-1000:])\n",
        "                                raise Exception(\n",
        "                                    f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
        "                                )\n",
        "\n",
        "                            # Add tool response\n",
        "                            traj.messages_and_choices.append(\n",
        "                                {\n",
        "                                    \"role\": \"tool\",\n",
        "                                    \"tool_call_id\": tool_call.id,\n",
        "                                    \"content\": content_text,\n",
        "                                }\n",
        "                            )\n",
        "\n",
        "                    except Exception as e:\n",
        "                        traceback.print_exc()\n",
        "                        traj.logs.append(f\"Tool call error: {e}\")\n",
        "\n",
        "                        # Add error response\n",
        "                        traj.messages_and_choices.append(\n",
        "                            {\n",
        "                                \"role\": \"tool\",\n",
        "                                \"tool_call_id\": tool_call.id,\n",
        "                                \"content\": f\"Error: {str(e)}\",\n",
        "                            }\n",
        "                        )\n",
        "            else:\n",
        "                # No tool calls â€” log and continue (RULER will likely give 0)\n",
        "                log(\n",
        "                    \"LLM returned no tool_calls; skipping tool execution\",\n",
        "                    turn=num_turns,\n",
        "                )\n",
        "                # You can consider breaking here or letting it try another turn\n",
        "                # break\n",
        "\n",
        "        except Exception as e:\n",
        "            traceback.print_exc()\n",
        "            traj.logs.append(f\"Error in turn {num_turns}: {e}\")\n",
        "            break\n",
        "\n",
        "    if not task_completed and num_turns == scenario.max_turns:\n",
        "        traj.metrics[\"ran_out_of_turns\"] = True\n",
        "\n",
        "    traj.metrics[\"num_turns\"] = num_turns\n",
        "\n",
        "    return traj.finish()\n",
        "\n",
        "\n",
        "# =============== Training code ===============\n",
        "\n",
        "print(\n",
        "    f\"Using config: max_turns={MAX_TURNS}, rollouts_per_group={TRAINING_CONFIG['rollouts_per_group']}, \"\n",
        "    f\"groups_per_step={TRAINING_CONFIG['groups_per_step']}, num_epochs={TRAINING_CONFIG['num_epochs']}, \"\n",
        "    f\"learning_rate={TRAINING_CONFIG['learning_rate']}\"\n",
        ")\n",
        "\n",
        "await model.register(backend)\n",
        "\n",
        "train_scenarios = [\n",
        "    McpScenario(\n",
        "        task_description=scenario[\"task\"],\n",
        "        max_turns=MAX_TURNS,\n",
        "    )\n",
        "    for scenario in raw_train_scenarios\n",
        "]\n",
        "\n",
        "# Create dataset iterator using raw scenarios\n",
        "train_iterator = iterate_dataset(\n",
        "    train_scenarios,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),  # Resume from checkpoint\n",
        ")\n",
        "\n",
        "# Main training loop using iterate_dataset\n",
        "for batch in train_iterator:\n",
        "    print(\"Gathering trajectory groups with RULER scoring...\")\n",
        "\n",
        "    # Use gather_trajectory_groups with ruler_score_group\n",
        "    groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(model, scenario, False)\n",
        "                for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "            )\n",
        "            for scenario in batch.items\n",
        "        ),\n",
        "        pbar_desc=f\"train gather step {batch.step}\",\n",
        "    )\n",
        "\n",
        "    scored_groups = []\n",
        "    for group in groups:\n",
        "        # Use RULER to assign relative scores to each trajectory\n",
        "        judged_group = await ruler_score_group(\n",
        "            group, judge_model=RULER_MODEL, debug=True, swallow_exceptions=True\n",
        "        )\n",
        "        scored_groups.append(judged_group)\n",
        "\n",
        "    print(\"starting train\")\n",
        "    await model.train(\n",
        "        scored_groups,\n",
        "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YRO9ndqo5ky4"
      },
      "outputs": [],
      "source": [
        "# @title Test Your Model!\n",
        "\n",
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "val_scenarios = [\n",
        "    McpScenario(\n",
        "        task_description=scenario[\"task\"],\n",
        "        max_turns=MAX_TURNS,\n",
        "    )\n",
        "    for scenario in raw_val_scenarios\n",
        "]\n",
        "\n",
        "print(f\"\\nðŸ§ª Testing the trained model on {len(val_scenarios)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, scenario in enumerate(val_scenarios):\n",
        "    print(f\"\\nTest {i + 1}:\")\n",
        "    print(f\"Input: {scenario.task_description}\")\n",
        "\n",
        "    # Run the model\n",
        "    result_trajectory = await rollout(model, scenario)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1][\"content\"] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Testing completed!\")\n",
        "print(\n",
        "    f\"\\nYour model '{MODEL_NAME}' has been trained to use the Smithery MCP server at:\"\n",
        ")\n",
        "print(SMITHERY_MCP_URL)\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\n",
        "    \"3. Or continue training with more examples by adjusting the configuration at the top\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "utI-VYM8s5lo"
      },
      "outputs": [],
      "source": [
        "# @title Upload to Hugging Face ðŸ¤—\n",
        "\n",
        "# Adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks), licensed under GNU LGPL v3.0.\n",
        "# Â© Unsloth contributors. Modifications Â© 2025 OpenPipe, Inc.\n",
        "# See THIRD-PARTY-NOTICES and licenses/LGPL-3.0.txt for details.\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "lora_model_path = (\n",
        "    f\".art/{model.project}/models/{model.name}/{await model.get_step():04d}\"\n",
        ")\n",
        "\n",
        "peft_model, peft_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=lora_model_path,\n",
        "    max_seq_length=16384,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "UPLOAD_MODEL = False  # Set True when you're ready to upload your model to Hugging Face\n",
        "HF_ACCOUNT = \"your_hf_account\"\n",
        "HF_TOKEN = \"your_hf_token\"\n",
        "\n",
        "if UPLOAD_MODEL:\n",
        "    peft_model.push_to_hub_merged(\n",
        "        f\"{HF_ACCOUNT}/{model.name}\", peft_tokenizer, token=HF_TOKEN\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuevYgXT-I1h"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A pre-built MCP server\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **MCP server refinement**: Add better tools and resources to the server\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your MCP server alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
