{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "To train this document summarization agent, click **Runtime** > **Run all**. Don't forget to set your environment variables.\n\n[![GitHub](https://img.shields.io/badge/GitHub-ART-blue?logo=github)](https://github.com/OpenPipe/ART)\n[![Discord](https://img.shields.io/badge/Discord-Join-7289da?logo=discord&logoColor=white)](https://discord.gg/zbBHRUpwf4)\n[![Docs](https://img.shields.io/badge/Docs-ART-green)](https://art.openpipe.ai)\n\n**Document Summarization Agent**\n\nIn this notebook, you will use [ART](https://github.com/openpipe/art) to train a document summarization agent using SFT+RL (supervised fine-tuning followed by reinforcement learning). First, the model is warmed up via distillation from a larger teacher model (SFT), then it is further improved through RL with a judge-based reward signal.\n\nThe agent learns to summarize documents into 350 words or less while maximizing the number of questions that can be answered from the summary alone. Documents come from the [Repliqa](https://huggingface.co/datasets/ServiceNow/repliqa) dataset, and [Gemini 3 Flash](https://ai.google.dev/gemini-api/docs/models#gemini-3-flash) is used as a judge to evaluate whether questions can be answered correctly from the summary.\n\n**Baselines** (average % of questions answered correctly from summary):\n- GPT-4o: 38%\n- GPT-4.1: 45%\n- Gemini-2.5-pro: 36%\n- Claude Sonnet-4: 57%\n\nThe goal is to train a model that outperforms all of them.\n\nNow let's get started!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install openpipe-art datasets regex async-lru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "This notebook uses the **ServerlessBackend** for training and inference, which requires a Weights & Biases API key.\n",
    "\n",
    "We also need an **OpenRouter API key** to access Gemini 3 Flash, which serves as the judge model for evaluating summary quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Required for Gemini 3 Flash judge model\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"\"\n",
    "\n",
    "# Required for serverless training\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"OPENROUTER_API_KEY is required for the Gemini 3 Flash judge model.\"\n",
    "    )\n",
    "\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"WANDB_API_KEY is required for inference, training, and logging to Weights & Biases.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Documents\n",
    "\n",
    "We use the [Repliqa](https://huggingface.co/datasets/ServiceNow/repliqa) dataset, which contains 3591 documents each paired with 5 questions and answers. The dataset is split into a validation set (91 documents) and a training set (3500 documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    q: str\n",
    "    a: str\n",
    "\n",
    "\n",
    "class Document(BaseModel):\n",
    "    document_text: str\n",
    "    questions: List[Question]\n",
    "\n",
    "\n",
    "def load_documents(\n",
    "    val_size: int = 91, train_size: int = 3500\n",
    ") -> Tuple[List[Document], List[Document]]:\n",
    "    ds = load_dataset(\"ServiceNow/repliqa\")\n",
    "    documents: Dict[str, Document] = {}\n",
    "\n",
    "    for data in ds[\"repliqa_0\"]:\n",
    "        if data[\"document_id\"] not in documents:\n",
    "            documents[data[\"document_id\"]] = Document(\n",
    "                document_text=data[\"document_extracted\"],\n",
    "                questions=[],\n",
    "            )\n",
    "        documents[data[\"document_id\"]].questions.append(\n",
    "            Question(q=data[\"question\"], a=data[\"answer\"])\n",
    "        )\n",
    "\n",
    "    all_documents = list(documents.values())\n",
    "\n",
    "    random.seed(80)\n",
    "    random.shuffle(all_documents)\n",
    "\n",
    "    if train_size + val_size > len(all_documents):\n",
    "        raise ValueError(\n",
    "            f\"Train size + val size ({train_size + val_size}) is greater than \"\n",
    "            f\"the total number of documents ({len(all_documents)})\"\n",
    "        )\n",
    "\n",
    "    val_documents = all_documents[:val_size]\n",
    "    train_documents = all_documents[val_size : val_size + train_size]\n",
    "\n",
    "    print(f\"Loaded {len(all_documents)} documents\")\n",
    "    print(f\"Train set size: {len(train_documents)}\")\n",
    "    print(f\"Val set size: {len(val_documents)}\")\n",
    "\n",
    "    return val_documents, train_documents\n",
    "\n",
    "\n",
    "val_documents, train_documents = load_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "We'll use a Qwen 3 14B model as the base, trained via the **ServerlessBackend** which handles GPU provisioning, inference, and training through Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art.serverless import ServerlessBackend\n",
    "\n",
    "backend = ServerlessBackend()\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"summarizer-002\",\n",
    "    project=\"summarize\",\n",
    "    base_model=\"OpenPipe/Qwen3-14B-Instruct\",\n",
    ")\n",
    "\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT Warmup via Distillation\n",
    "\n",
    "Before RL training, we warm up the student model by distilling from a larger teacher model (**Qwen3-235B**). The teacher generates summaries for the first 200 training documents, and the student learns to imitate them via supervised fine-tuning. These 200 documents will be excluded from the subsequent RL training to avoid data overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import asyncio\n\nfrom openai import AsyncOpenAI\n\nTEACHER_MODEL = \"Qwen/Qwen3-235B-A22B-Instruct-2507\"\nSFT_NUM_EXAMPLES = 200\n\nteacher_client = AsyncOpenAI(\n    api_key=os.environ[\"WANDB_API_KEY\"],\n    base_url=\"https://api.inference.wandb.ai/v1\",\n)\n\nsft_documents = train_documents[:SFT_NUM_EXAMPLES]\n\nsystem_prompt = \"You are a specialized AI assistant that generates concise, informative summaries for documents.\"\n\n\nasync def get_teacher_completion(document: Document) -> art.Trajectory:\n    summarize_prompt = (\n        f\"Here is a document: {document.document_text}\\n\\n\"\n        \"Generate a summary that conveys all relevant information in a concise manner.\"\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": summarize_prompt},\n    ]\n    completion = await teacher_client.chat.completions.create(\n        model=TEACHER_MODEL,\n        messages=messages,\n        max_tokens=1000,\n    )\n    response = completion.choices[0].message.content\n    print(f\"Generated summary ({len(response.split())} words)\")\n    return art.Trajectory(\n        messages_and_choices=[\n            *messages,\n            {\"role\": \"assistant\", \"content\": response},\n        ],\n    )\n\n\n# Generate teacher completions for the first 200 training documents\nsft_trajectories = await asyncio.gather(\n    *[get_teacher_completion(doc) for doc in sft_documents]\n)\nsft_trajectories = list(sft_trajectories)\n\nprint(f\"\\nGenerated {len(sft_trajectories)} teacher trajectories for SFT warmup.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.utils.sft import create_sft_dataset_iterator\n",
    "\n",
    "for chunk in create_sft_dataset_iterator(sft_trajectories, peak_lr=2e-4):\n",
    "    await model.train_sft(chunk.trajectories, chunk.config)\n",
    "\n",
    "print(\"SFT warmup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge Model\n",
    "\n",
    "We use **Gemini 3 Flash** (via OpenRouter) as the judge model. For each question, the judge:\n",
    "1. Answers the question using only the summary\n",
    "2. Answers the question using the full document (as a reference)\n",
    "3. Compares both answers and scores the summary-based answer as correct (1) or incorrect (0)\n",
    "\n",
    "Responses are cached (up to 1024 entries) to avoid redundant API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from async_lru import alru_cache\n\njudge_semaphore = asyncio.Semaphore(20)\n\njudge_client = AsyncOpenAI(\n    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n    base_url=\"https://openrouter.ai/api/v1\",\n)\n\n\n@alru_cache(maxsize=1024)\nasync def get_judge_completion(\n    prompt, temperature=0.0, max_tokens=600, retries=3\n) -> str:\n    for attempt in range(1, retries + 1):\n        try:\n            async with judge_semaphore:\n                completion = await judge_client.chat.completions.create(\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    model=\"google/gemini-3-flash-preview\",\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                )\n            return completion.choices[0].message.content.strip()\n        except Exception as e:\n            if attempt < retries:\n                print(\n                    f\"[Retry {attempt}/{retries}] get_judge_completion failed: {e}. Retrying...\"\n                )\n                await asyncio.sleep(3)\n            else:\n                print(\n                    f\"[Failure] get_judge_completion failed after {retries} attempts: {e}\"\n                )\n                return \"ERROR: Get judge completion failed\"\n\n\ndef clear_judge_cache():\n    get_judge_completion.cache_clear()\n    print(\"Judge cache cleared\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Rollout\n",
    "\n",
    "A rollout is a single episode of the agent performing its task. The process:\n",
    "\n",
    "1. The model receives a document and generates a summary\n",
    "2. For each of the 5 questions associated with the document:\n",
    "   - Gemini 3 Flash answers the question using **only the summary**\n",
    "   - Gemini 3 Flash answers the question using the **full document** (reference)\n",
    "   - A judge compares both answers and scores 0 or 1\n",
    "3. The **reward** is the total number of correctly answered questions (0-5)\n",
    "\n",
    "The agent never sees the questions during summarization - it must learn to write summaries that capture all important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import openai\nimport regex\n\n\nclass SummarizerScenario(BaseModel):\n    doc: Document\n\n\n@art.retry(exceptions=(openai.LengthFinishReasonError,))\nasync def rollout(\n    model: art.Model, scenario: SummarizerScenario\n) -> art.Trajectory:\n    client = model.openai_client()\n\n    trajectory = art.Trajectory(\n        messages_and_choices=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a specialized AI assistant that generates concise, informative summaries for documents.\",\n            }\n        ],\n        reward=0,\n        metrics={\n            \"word_count\": 0,\n            \"len\": 0,\n            \"percent\": 0,\n            \"percent_full\": 0,\n            \"percent_diff\": 0,\n        },\n    )\n\n    summarize_prompt = (\n        f\"Here is a document: {scenario.doc.document_text}\\n\\n\"\n        \"Generate a summary that conveys all relevant information in a concise manner.\"\n    )\n\n    trajectory.messages_and_choices.append(\n        {\"role\": \"user\", \"content\": summarize_prompt}\n    )\n\n    messages = trajectory.messages()\n    completion = await client.chat.completions.create(\n        model=model.get_inference_name(), messages=messages, max_tokens=1000\n    )\n    choice = completion.choices[0]\n    trajectory.messages_and_choices.append(choice)\n    summary = choice.message.content\n\n    total_score = 0\n    total_score_full = 0\n    total_questions = 0\n\n    for question in scenario.doc.questions:\n        total_questions += 1\n\n        # Score from summary (skip if summary contains Chinese characters or is too long)\n        if not regex.search(r\"\\p{Han}\", summary) and len(summary) <= 3000:\n            prompt = (\n                f\"Here is a document: {summary}\\n\\n\"\n                f\"Answer this question to the best of your ability in one sentence, \"\n                f\"if the document does not contain the answer, just state so: {question.q}\"\n            )\n            response = await get_judge_completion(prompt)\n\n            judge_prompt = (\n                f\"Here is a document: {scenario.doc.document_text}\\n\\n\"\n                f\"Here is a question: {question.q}\\n\\n\"\n                f\"Here is a generated answer: {response}\\n\\n\"\n                f\"Here is the golden answer: {question.a}\\n\\n\"\n                \"If the answers mostly match return a 1, if they do not match return a 0. \"\n                \"Do not return any other text.\"\n            )\n            score = await get_judge_completion(judge_prompt)\n            try:\n                total_score += int(score)\n            except:\n                pass\n\n        # Score from full document (reference)\n        prompt_full = (\n            f\"Here is a document: {scenario.doc.document_text}\\n\\n\"\n            f\"Answer this question to the best of your ability in one sentence, \"\n            f\"if the document does not contain the answer, just state so: {question.q}\"\n        )\n        response_full = await get_judge_completion(prompt_full)\n\n        judge_prompt_full = (\n            f\"Here is a document: {scenario.doc.document_text}\\n\\n\"\n            f\"Here is a question: {question.q}\\n\\n\"\n            f\"Here is a generated answer: {response_full}\\n\\n\"\n            f\"Here is the golden answer: {question.a}\\n\\n\"\n            \"If the answers mostly match return a 1, if they do not match return a 0. \"\n            \"Do not return any other text.\"\n        )\n        score_full = await get_judge_completion(judge_prompt_full)\n        try:\n            total_score_full += int(score_full)\n        except:\n            pass\n\n        # Debug logging (5% sample)\n        if not regex.search(r\"\\p{Han}\", summary) and len(summary) <= 3000:\n            if random.random() < 0.05:\n                print(f\"Question: {question.q}\")\n                print(f\"Golden: {question.a}\")\n                print(f\"Generated: {response}\")\n                print(f\"Score: {score}, Score-full: {score_full}\")\n                print()\n\n    trajectory.metrics[\"percent\"] = total_score / total_questions\n    trajectory.metrics[\"percent_full\"] = total_score_full / total_questions\n    trajectory.metrics[\"percent_diff\"] = (\n        trajectory.metrics[\"percent\"] - trajectory.metrics[\"percent_full\"]\n    )\n    trajectory.metrics[\"word_count\"] = len(summary.split())\n    trajectory.metrics[\"len\"] = len(summary)\n    trajectory.reward = total_score\n\n    return trajectory\n\n\nprint(\"Rollout function defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Training Loop\n",
    "\n",
    "Now we continue with reinforcement learning. The first 200 documents used for SFT distillation are excluded to avoid data overlap, leaving 3300 documents for RL training.\n",
    "\n",
    "For each training step:\n",
    "1. Generate training rollouts (4 per batch document)\n",
    "2. Every 10 steps, also generate validation rollouts (2 per validation document) and log metrics\n",
    "3. Train the model on the training trajectories\n",
    "\n",
    "We use `iterate_dataset` to handle batching and epoch management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from art.utils import iterate_dataset\n\nGROUPS_PER_STEP = 5\nROLLOUTS_PER_GROUP = 4\nVAL_ROLLOUTS_PER_GROUP = 2\nVAL_STEP_INTERVAL = 10\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 1\nMAX_RL_STEPS = 1000\n\n# Skip the documents used for SFT distillation\nrl_train_documents = train_documents[SFT_NUM_EXAMPLES:]\nrl_start_step = await model.get_step()\nprint(f\"RL training on {len(rl_train_documents)} documents (excluding {SFT_NUM_EXAMPLES} used for SFT)\")\nprint(f\"Starting from step {rl_start_step}, will run {MAX_RL_STEPS} RL steps\")\n\ntraining_iterator = iterate_dataset(\n    rl_train_documents,\n    groups_per_step=GROUPS_PER_STEP,\n    num_epochs=NUM_EPOCHS,\n    initial_step=rl_start_step,\n)\n\nfor batch in training_iterator:\n    if batch.step >= rl_start_step + MAX_RL_STEPS:\n        break\n\n    print(\n        f\"Step {batch.step}, Epoch {batch.epoch}, \"\n        f\"Epoch step {batch.epoch_step}, \"\n        f\"Batch size {len(batch.items)}\"\n    )\n\n    # Generate training rollouts\n    train_groups = await art.gather_trajectory_groups(\n        (\n            art.TrajectoryGroup(\n                rollout(model, SummarizerScenario(doc=document))\n                for _ in range(ROLLOUTS_PER_GROUP)\n            )\n            for document in batch.items\n        ),\n        pbar_desc=f\"gather train (step {batch.step})\",\n    )\n\n    # Run validation every VAL_STEP_INTERVAL steps\n    if batch.step % VAL_STEP_INTERVAL == 0:\n        val_groups = await art.gather_trajectory_groups(\n            (\n                art.TrajectoryGroup(\n                    rollout(\n                        model,\n                        SummarizerScenario(doc=document),\n                    )\n                    for _ in range(VAL_ROLLOUTS_PER_GROUP)\n                )\n                for document in val_documents\n            ),\n            pbar_desc=f\"gather val (step {batch.step})\",\n        )\n        await model.log(val_groups, split=\"val\")\n\n    # Train on training trajectories\n    result = await backend.train(\n        model, train_groups, learning_rate=LEARNING_RATE\n    )\n    await model.log(\n        train_groups,\n        metrics=result.metrics,\n        step=result.step,\n        split=\"train\",\n    )\n\n    print(f\"Completed step {batch.step}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Model\n",
    "\n",
    "Try the trained model on a validation document to see how it summarizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_doc = val_documents[0]\n\nclient = model.openai_client()\ncompletion = await client.chat.completions.create(\n    model=model.get_inference_name(),\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a specialized AI assistant that generates concise, informative summaries for documents.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Here is a document: {test_doc.document_text}\\n\\n\"\n                \"Generate a summary that conveys all relevant information in a concise manner.\"\n            ),\n        },\n    ],\n    max_tokens=1000,\n)\n\nsummary = completion.choices[0].message.content\nprint(f\"Summary ({len(summary.split())} words):\\n\")\nprint(summary)\nprint(\"\\n--- Questions for this document ---\")\nfor i, q in enumerate(test_doc.questions, 1):\n    print(f\"\\n{i}. {q.q}\")\n    print(f\"   Answer: {q.a}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking (Optional)\n",
    "\n",
    "Uncomment and run the cell below to benchmark SOTA models against your trained model on the validation set. This requires an `OPENROUTER_API_KEY` to access the models via OpenRouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_NAME = \"summarize\"\n",
    "# BENCHMARK_ROLLOUTS = 2\n",
    "#\n",
    "# # Define baseline models (all accessed via OpenRouter)\n",
    "# gpt_4o = art.Model(\n",
    "#     name=\"gpt-4o\",\n",
    "#     project=PROJECT_NAME,\n",
    "#     inference_model_name=\"openai/gpt-4o\",\n",
    "#     inference_api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "#     inference_base_url=\"https://openrouter.ai/api/v1\",\n",
    "# )\n",
    "#\n",
    "# gpt_4_1 = art.Model(\n",
    "#     name=\"gpt-4.1\",\n",
    "#     project=PROJECT_NAME,\n",
    "#     inference_model_name=\"openai/gpt-4.1\",\n",
    "#     inference_api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "#     inference_base_url=\"https://openrouter.ai/api/v1\",\n",
    "# )\n",
    "#\n",
    "# gemini_2_5_pro = art.Model(\n",
    "#     name=\"gemini-2.5-pro\",\n",
    "#     project=PROJECT_NAME,\n",
    "#     inference_model_name=\"google/gemini-2.5-pro-preview\",\n",
    "#     inference_api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "#     inference_base_url=\"https://openrouter.ai/api/v1\",\n",
    "# )\n",
    "#\n",
    "# sonnet_4 = art.Model(\n",
    "#     name=\"sonnet-4\",\n",
    "#     project=PROJECT_NAME,\n",
    "#     inference_model_name=\"anthropic/claude-sonnet-4\",\n",
    "#     inference_api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
    "#     inference_base_url=\"https://openrouter.ai/api/v1\",\n",
    "# )\n",
    "#\n",
    "#\n",
    "# async def benchmark_model(bm_model: art.Model) -> None:\n",
    "#     trajectory_groups = await art.gather_trajectory_groups(\n",
    "#         (\n",
    "#             art.TrajectoryGroup(\n",
    "#                 rollout(bm_model, SummarizerScenario(doc=document))\n",
    "#                 for _ in range(BENCHMARK_ROLLOUTS)\n",
    "#             )\n",
    "#             for document in val_documents\n",
    "#         ),\n",
    "#         pbar_desc=bm_model.name,\n",
    "#     )\n",
    "#     await bm_model.log(trajectories=trajectory_groups, split=\"val\")\n",
    "#\n",
    "#\n",
    "# benchmark_models = [gpt_4o, gpt_4_1, gemini_2_5_pro, sonnet_4]\n",
    "# for bm_model in benchmark_models:\n",
    "#     await bm_model.register(backend)\n",
    "#\n",
    "# # Benchmark all models simultaneously\n",
    "# await asyncio.gather(*[benchmark_model(bm_model) for bm_model in benchmark_models])\n",
    "#\n",
    "# print(\"Benchmarking complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}